{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pred_LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KvrqC4FtuCQ",
        "outputId": "652efb8e-0a49-400e-f5a0-8e0e41ce1f9e"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U flask-cors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCVGF-qdvZFb",
        "outputId": "92c5169e-d08e-4b03-afd6-4c02910ad0c5"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.7/dist-packages (3.0.10)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.7/dist-packages (from flask-cors) (1.1.4)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors) (1.15.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.9->flask-cors) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask,request,render_template,url_for,jsonify\n",
        "from flask_cors import CORS, cross_origin\n",
        "import site\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "P62TODYGtwx4"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating word tokens after removing characters other than alphabets, converting them to lower case and\n",
        "# removing stopwords from the text'''\n",
        "\n",
        "def word_tokens(essay_text):\n",
        "    essay_text = re.sub(\"[^a-zA-Z]\", \" \", essay_text)\n",
        "    words = essay_text.lower().split()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    return (words)"
      ],
      "metadata": {
        "id": "BH2TTEEIr_oh"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating sentence tokens from the essay and finally the word tokens\n",
        "\n",
        "def sentence_tokens(essay_text):\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    sent_tokens = tokenizer.tokenize(essay_text.strip())\n",
        "    sentences = []\n",
        "    for sent_token in sent_tokens:\n",
        "        if len(sent_token) > 0:\n",
        "            sentences.append(word_tokens(sent_token))\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "mYo_h6Q9sAl7"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a vector of features\n",
        "\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    num_words = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            num_words += 1\n",
        "            featureVec = np.add(featureVec,model[word])        \n",
        "    featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec"
      ],
      "metadata": {
        "id": "7wcULoaLsAoa"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating word vectors to be used in word2vec model\n",
        "\n",
        "def getAvgFeatureVecs(essays, model, num_features):\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for essay_text in essays:\n",
        "        essayFeatureVecs[counter] = makeFeatureVec(essay_text, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs"
      ],
      "metadata": {
        "id": "8AywQ9vmsAqr"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "787tMMsqsAuH"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertToVec(text):\n",
        "    content=text\n",
        "    if len(content) > 20:\n",
        "        num_features = 300\n",
        "        model = KeyedVectors.load_word2vec_format(\"/content/word2vecmodel_lstm.bin\", binary=True)\n",
        "        clean_test_essays = []\n",
        "        clean_test_essays.append(word_tokens(content))\n",
        "        testDataVecs = getAvgFeatureVecs(clean_test_essays, model, num_features )\n",
        "        testDataVecs = np.array(testDataVecs)\n",
        "        testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "        lstm_model = load_model(\"/content/LSTM.h5\")\n",
        "        preds = lstm_model.predict(testDataVecs)\n",
        "        print(preds)\n",
        "        return str(round(preds[0][0]))\n"
      ],
      "metadata": {
        "id": "CCUKj8zVrwQA"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "iVYCzozRrNds"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_task():\n",
        "    K.clear_session()\n",
        "    test = input(\"Enter eassy : \") \n",
        "    #test=\"The Newspaper is one of the oldest means of communication which provides information from all around the world.\"\n",
        "    score = convertToVec(test)\n",
        "    K.clear_session()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc=create_task()\n",
        "print(\"Score : \",sc)"
      ],
      "metadata": {
        "id": "gmv8phcXruEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082ff229-ad69-4ed6-f49c-de29e02111b8"
      },
      "execution_count": 130,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter eassy : I think the computer dire a postive effect on people all over the world. You can do lots of things on the computer like an acidemic work, talk on the enernet, or even find out facts' about fam'us people you love, people computer. I think computer can benfit socity becuse computers can help you with your acidimic work, and can also talk to you friends online. One way i can benfit from using the computer by, typing my school work. But when in not doing my school work talking to my friends online. But the best thing of all is finding out information about famuse people like @PERSON1. I rember when is was doing a project on the computer and got a @NUM1 on it becuse i used the computer. That should prove that computers are very help full. Have you ever wondered how a computer can help you in your ever day life. If you said no i can tell you how. You can learn about diffrent places and food. Lets say i was going to make pizza and i did not know how i would have to look on the computer how to. A professor from virgin new york tech school said that with out computers the world is nothing. @PERCENT1 of people at new york tech school agreed but @PERCENT2 did not could you immagin a world with out computers. I if the world did not have computers we would not have cell phones or even t.v. Not ever on know the cell phones or computers lets face it with out computers the world would be lost like the proffser from new york tech said. Also teenagrs would be bored becuse most teenangers do not like to go out side and if we do not have computers how will adults do there work or they will have to handwrite it. Lets be honsest some adults and kids hand writing is like chicken scrach so to say. I think computer are very helpful for everyone around the world. In closing i think the hole world should keep computers becus with out computers the world is lost, to the past paper.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7.882696]]\n",
            "Score :  8\n"
          ]
        }
      ]
    }
  ]
}