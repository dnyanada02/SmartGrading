{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pred_BiLSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "7KvrqC4FtuCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U flask-cors"
      ],
      "metadata": {
        "id": "zCVGF-qdvZFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask,request,render_template,url_for,jsonify\n",
        "from flask_cors import CORS, cross_origin\n",
        "import site\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "P62TODYGtwx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating word tokens after removing characters other than alphabets, converting them to lower case and\n",
        "# removing stopwords from the text'''\n",
        "\n",
        "def word_tokens(essay_text):\n",
        "    essay_text = re.sub(\"[^a-zA-Z]\", \" \", essay_text)\n",
        "    words = essay_text.lower().split()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    return (words)"
      ],
      "metadata": {
        "id": "BH2TTEEIr_oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating sentence tokens from the essay and finally the word tokens\n",
        "\n",
        "def sentence_tokens(essay_text):\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    sent_tokens = tokenizer.tokenize(essay_text.strip())\n",
        "    sentences = []\n",
        "    for sent_token in sent_tokens:\n",
        "        if len(sent_token) > 0:\n",
        "            sentences.append(word_tokens(sent_token))\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "mYo_h6Q9sAl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a vector of features\n",
        "\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    num_words = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            num_words += 1\n",
        "            featureVec = np.add(featureVec,model[word])        \n",
        "    featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec"
      ],
      "metadata": {
        "id": "7wcULoaLsAoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating word vectors to be used in word2vec model\n",
        "\n",
        "def getAvgFeatureVecs(essays, model, num_features):\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for essay_text in essays:\n",
        "        essayFeatureVecs[counter] = makeFeatureVec(essay_text, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs"
      ],
      "metadata": {
        "id": "8AywQ9vmsAqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "787tMMsqsAuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertToVec(text):\n",
        "    content=text\n",
        "    if len(content) > 20:\n",
        "        num_features = 300\n",
        "        model = KeyedVectors.load_word2vec_format(\"/content/word2vecmodel_bilstm.bin\", binary=True)\n",
        "        clean_test_essays = []\n",
        "        clean_test_essays.append(word_tokens(content))\n",
        "        testDataVecs = getAvgFeatureVecs(clean_test_essays, model, num_features )\n",
        "        testDataVecs = np.array(testDataVecs)\n",
        "        testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "        lstm_model = load_model(\"/content/Bi_LSTM.h5\")\n",
        "        preds = lstm_model.predict(testDataVecs)\n",
        "        print(preds)\n",
        "        return str(round(preds[0][0]))\n"
      ],
      "metadata": {
        "id": "CCUKj8zVrwQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVYCzozRrNds"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_task():\n",
        "    K.clear_session()\n",
        "    test = input(\"Enter eassy : \") \n",
        "    #test=\"The Newspaper is one of the oldest means of communication which provides information from all around the world.\"\n",
        "    score = convertToVec(test)\n",
        "    K.clear_session()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc=create_task()\n",
        "print(\"Score : \",sc)"
      ],
      "metadata": {
        "id": "gmv8phcXruEz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}